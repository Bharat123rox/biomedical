{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d054698",
   "metadata": {},
   "source": [
    "# IOB Dataset Prompts\n",
    "\n",
    "For sequence tagging datasets, sometimes it's easier to define a pure python prompting template vs. using the Jinja-based promptsource tools. A fair number of biomedical NER datasets are provided in IOB format, so these prompts should be transferable across other entity types. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6030abe",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "Some HuggingFace datasets are provided in tagged IOB format already. We'll load an example biomedical corpus (NCBI Disease Corpus) to illustrate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0998990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('ncbi_disease', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c787aa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset_builder\n",
    "dataset_builder = load_dataset_builder('ncbi_disease')\n",
    "\n",
    "dftrs = dataset_builder.info.features\n",
    "dsplits = dataset_builder.info.splits\n",
    "\n",
    "class_labels = dftrs['ner_tags'].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0451dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from itertools import groupby\n",
    "\n",
    "def get_spans(toks, tags, class_labels):\n",
    "    \"\"\"\n",
    "    Assume sentences that are tokenized and stored  as lists of: \n",
    "        `tokens` and `ner_tags` in IOB format. \n",
    "    For tag labels, we assume entity types are contiguous and that \n",
    "    the head token tags (B-*) labels are the minimal value within \n",
    "    an entity type. \n",
    "    \n",
    "    For example for IOB tagging\n",
    "        {0:'O', 1:'B-Disease', 2:'I-Disease', 3:'B-Chemical', 4:'I-Chemical'}\n",
    "  \n",
    "    NOTE: This does not support BILUO tagging like spaCy\n",
    "    \"\"\"\n",
    "    idx = 0\n",
    "    types = []\n",
    "    spans = []\n",
    "    \n",
    "    # get head class label for each entity type\n",
    "    entity_labels = {}\n",
    "    for y in class_labels:\n",
    "        entity_labels[class_labels[y]] = min(entity_labels[class_labels[y]], y) if class_labels[y] in entity_labels else y\n",
    " \n",
    "    for i, j in groupby(tags):\n",
    "        chunk = list(j)\n",
    "        tokens = toks[idx:idx+len(chunk)]\n",
    "        cls = chunk[0]\n",
    "        entity_type = class_labels[cls]\n",
    "        # non-entity tag\n",
    "        if entity_type == 'O' or len(spans) == 0:\n",
    "            spans.append(tokens)\n",
    "            types.append(entity_type)\n",
    "        # current token is I-tag and same type as B-tag\n",
    "        elif types[-1] == entity_type and cls >= entity_labels[entity_type]:\n",
    "            spans[-1].extend(tokens)\n",
    "        # otherwise is B-tag entity\n",
    "        else:\n",
    "            spans.append(tokens)\n",
    "            types.append(entity_type)\n",
    "        idx += len(chunk)\n",
    "        \n",
    "    spans = [' '.join(s) for s in spans]\n",
    "    return list(zip(types, spans))\n",
    "\n",
    "# example\n",
    "class_labels = {0:'O', 1:'disease', 2:'disease'}\n",
    "entity_spans = get_spans(dataset[11]['tokens'], dataset[11]['ner_tags'], class_labels)\n",
    "entity_spans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1935d4ad",
   "metadata": {},
   "source": [
    "## Prompts as Python Functions\n",
    "This is hacky and unpolished, but it works fine for something fast. See the [promptsource](https://github.com/bigscience-workshop/promptsource) contribution guide for more details around designing prompts.\n",
    "\n",
    "NOTE: With sequence tagging tasks, we have to get a little creative when specificying tasks. It's unclear what the best stategy is for NER tasks and prompts. Some lessons learned from the initial prompt source experiments\n",
    "\n",
    "- Performance is better if you generate the text from a span vs. some aux. attribute like the index of a span\n",
    "- Outputs need to minimize ambiguity, e.g., when generating a list of outputs, using some human-like delimiters of commas, newlines, etc. \n",
    "- More templates per task is good for evaluation and other research inquiries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc55ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prompt_list_of_disease_entities_v1(x):\n",
    "    class_labels = {0:'O', 1:'disease', 2:'disease'}\n",
    "    entity_spans = get_spans(x['tokens'], x['ner_tags'], class_labels)\n",
    "    sentence = ' '.join(x['tokens']).strip()\n",
    "    # disease entities\n",
    "    target = ', '.join([span[-1] for span in entity_spans if span[0] == 'disease'])\n",
    "    if not target:\n",
    "        target = 'None'\n",
    "    \n",
    "    tmpl = \"Create a comma-separated list of all disease named entities found in the following sentence. \"\n",
    "    tmpl += \"If there are no disease mentions, print None. \\n\"\n",
    "    tmpl += f\"Sentence: {sentence}\\nEntities:|||{target}\"\n",
    "    return tmpl\n",
    "\n",
    "def prompt_list_of_disease_entities_v2(x):\n",
    "    \"\"\"\n",
    "    answers in prompt: ...\n",
    "    original task: no\n",
    "    \"\"\"\n",
    "    class_labels = {0:'O', 1:'disease', 2:'disease'}\n",
    "    entity_spans = get_spans(x['tokens'], x['ner_tags'], class_labels)\n",
    "    sentence = ' '.join(x['tokens']).strip()\n",
    "    # disease entities\n",
    "    target = ', '.join([span[-1] for span in entity_spans if span[0] == 'disease'])\n",
    "    if not target:\n",
    "        target = 'None'\n",
    "    \n",
    "    tmpl = \"Identify all disease names mentioned in the following sentence. \"\n",
    "    tmpl += \"If there are no disease mentions, print None. \\n\"\n",
    "    tmpl += f\"\\\"{sentence}\\\"\\n|||{target}\"\n",
    "    return tmpl\n",
    "\n",
    "\n",
    "def prompt_list_of_disease_entities_v3(x):\n",
    "    class_labels = {0:'O', 1:'disease', 2:'disease'}\n",
    "    entity_spans = get_spans(x['tokens'], x['ner_tags'], class_labels)\n",
    "    sentence = ' '.join(x['tokens']).strip()\n",
    "    # disease entities\n",
    "    target = '\\n'.join([f\"- {span[-1]}\" for span in entity_spans if span[0] == 'disease'])\n",
    "    if not target:\n",
    "        target = 'None'\n",
    "    \n",
    "    tmpl = \"Create a bulleted list of all disease named entities found in the following sentence. \"\n",
    "    tmpl += \"If there are no disease mentions, print None. \\n\"\n",
    "    tmpl += f\"\\\"{sentence}\\\"\\n|||{target}\"\n",
    "    return tmpl\n",
    "\n",
    "def prompt_list_of_disease_entities_v4(x):\n",
    "    class_labels = {0:'O', 1:'disease', 2:'disease'}\n",
    "    entity_spans = get_spans(x['tokens'], x['ner_tags'], class_labels)\n",
    "    sentence = ' '.join(x['tokens']).strip()\n",
    "    # disease entities\n",
    "    target = '\\n'.join([span[-1] for span in entity_spans if span[0] == 'disease'])\n",
    "    if not target:\n",
    "        target = 'None'\n",
    "    \n",
    "    tmpl = \"Create a list, separated by newlines, of all disease named entities found in the following sentence. \"\n",
    "    tmpl += \"If there are no disease mentions, print None. \\n\"\n",
    "    tmpl += f\"\\\"{sentence}\\\"\\n|||{target}\"\n",
    "    return tmpl\n",
    "\n",
    "\n",
    "def prompt_mention_of_disease_yes_no_answers(x):\n",
    "    \"\"\"\n",
    "    Transform NER task into binary sentence classification, \n",
    "    \"does this sentence contain an entity of type x\"\n",
    "    \n",
    "    answers in prompt: yes\n",
    "    original task: no\n",
    "    \"\"\"\n",
    "    class_labels = {0:'O', 1:'disease', 2:'disease'}\n",
    "    entity_spans = get_spans(x['tokens'], x['ner_tags'], class_labels)\n",
    "    sentence = ' '.join(x['tokens']).strip()\n",
    "    # disease entities\n",
    "    diseases = [span[-1] for span in entity_spans if span[0] == 'disease']\n",
    "    target = 'yes' if len(diseases) > 0 else 'no'\n",
    "   \n",
    "    tmpl = \"Does the following sentence contain mentions of disease names? yes or no \\n\"\n",
    "    tmpl += f\"\\\"{sentence}\\\"\\n|||{target}\"\n",
    "    return tmpl\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    try:\n",
    "        s = prompt_list_of_disease_entities_v4(dataset[i])\n",
    "        print(s)\n",
    "        print('-' * 80)\n",
    "    except Exception as e:\n",
    "        print('error', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9484b63c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomedical",
   "language": "python",
   "name": "biomedical"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
